{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Format all images and save in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from data_fixing import *\n",
    "from train_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000006.png, invalid shape: (128, 128)\n",
      "Skipping 00000116.png, invalid shape: (128, 128)\n",
      "Skipping 00000005.png, invalid shape: (128, 128)\n",
      "Skipping 00000150.png, invalid shape: (128, 128)\n",
      "Skipping 00000129.png, invalid shape: (128, 128)\n",
      "Skipping 00000229.png, invalid shape: (128, 128)\n",
      "Skipping 00000114.png, invalid shape: (128, 128)\n",
      "Skipping 00000190.png, invalid shape: (128, 128)\n",
      "Skipping 00000009.png, invalid shape: (128, 128)\n",
      "Skipping 00000021.png, invalid shape: (128, 128)\n",
      "Skipping 00000061.png, invalid shape: (128, 128)\n",
      "Skipping 00000078.png, invalid shape: (128, 128)\n",
      "Skipping 00000172.png, invalid shape: (128, 128)\n",
      "Skipping 59bef3942b6041b3a6e0526100264536.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000121.png, invalid shape: (128, 128)\n",
      "Skipping 00000230.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000156.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000020.png, invalid shape: (128, 128)\n",
      "Skipping 00000081.png, invalid shape: (128, 128)\n",
      "Skipping 00000149.png, invalid shape: (128, 128)\n",
      "Skipping 00000114.png, invalid shape: (128, 128)\n",
      "Skipping 00000144.png, invalid shape: (128, 128)\n",
      "Skipping 00000203.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000003.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000148.png, invalid shape: (128, 128)\n",
      "Skipping 00000020.png, invalid shape: (128, 128)\n",
      "Skipping 00000007.png, invalid shape: (128, 128)\n",
      "Skipping 00000061.png, invalid shape: (128, 128)\n",
      "Skipping 00000003.png, invalid shape: (128, 128)\n",
      "Skipping 00000010.png, invalid shape: (128, 128)\n",
      "Skipping 00000005.png, invalid shape: (128, 128)\n",
      "Skipping 00000014.png, invalid shape: (128, 128)\n",
      "Skipping 00000033.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000095.png, invalid shape: (128, 128)\n",
      "Skipping 00000111.png, invalid shape: (128, 128)\n",
      "Skipping 00000150.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000063.png, invalid shape: (128, 128)\n",
      "Skipping 00000200.png, invalid shape: (128, 128)\n",
      "Skipping 00000223.png, invalid shape: (128, 128)\n",
      "Skipping 00000003.png, invalid shape: (128, 128)\n",
      "Skipping 00000026.png, invalid shape: (128, 128)\n",
      "Skipping 00000045.png, invalid shape: (128, 128)\n",
      "Skipping 00000058.png, invalid shape: (128, 128)\n",
      "Skipping 00000071.png, invalid shape: (128, 128)\n",
      "Skipping 00000077.png, invalid shape: (128, 128)\n",
      "Skipping 00000114.png, invalid shape: (128, 128)\n",
      "Skipping 00000193.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000046.png, invalid shape: (128, 128)\n",
      "Skipping 00000067.png, invalid shape: (128, 128)\n",
      "Skipping 00000107.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000013.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000020.png, invalid shape: (128, 128)\n",
      "Skipping 00000143.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000011.png, invalid shape: (128, 128)\n",
      "Skipping 00000010.png, invalid shape: (128, 128)\n",
      "Skipping 00000077.png, invalid shape: (128, 128)\n",
      "Skipping 00000089.png, invalid shape: (128, 128)\n",
      "Skipping 00000103.png, invalid shape: (128, 128)\n",
      "Skipping 00000143.png, invalid shape: (128, 128)\n",
      "Skipping 00000180.png, invalid shape: (128, 128)\n",
      "Skipping 00000194.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000094.png, invalid shape: (128, 128)\n",
      "Skipping 00000224.png, invalid shape: (128, 128, 2)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000077.png, invalid shape: (128, 128)\n",
      "Skipping 00000087.png, invalid shape: (128, 128)\n",
      "Skipping 00000099.png, invalid shape: (128, 128)\n",
      "Skipping 00000110.png, invalid shape: (128, 128)\n",
      "Skipping 00000141.png, invalid shape: (128, 128)\n",
      "Skipping 00000226.png, invalid shape: (128, 128, 2)\n",
      "Skipping 00000235.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000185.png, invalid shape: (128, 128)\n",
      "Skipping 00000212.png, invalid shape: (128, 128)\n",
      "Skipping 00000020.png, invalid shape: (128, 128)\n",
      "Skipping 00000049.png, invalid shape: (128, 128)\n",
      "Skipping 00000065.png, invalid shape: (128, 128)\n",
      "Skipping 00000067.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000164.png, invalid shape: (128, 128)\n",
      "Skipping 00000226.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000064.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000028.png, invalid shape: (128, 128)\n",
      "Skipping 00000045.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000024.png, invalid shape: (128, 128)\n",
      "Skipping 00000041.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000137.png, invalid shape: (128, 128)\n",
      "Skipping 00000153.png, invalid shape: (128, 128)\n",
      "Skipping 00000219.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000070.png, invalid shape: (128, 128)\n",
      "Skipping 00000235.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000012.png, invalid shape: (128, 128)\n",
      "Skipping 00000033.png, invalid shape: (128, 128)\n",
      "Skipping 00000028.png, invalid shape: (128, 128)\n",
      "Skipping 00000021.png, invalid shape: (128, 128)\n",
      "Skipping 00000042.png, invalid shape: (128, 128)\n",
      "Skipping 00000012.png, invalid shape: (128, 128)\n",
      "Skipping 00000041.png, invalid shape: (128, 128)\n",
      "Skipping 00000176.png, invalid shape: (128, 128)\n",
      "Skipping 00000226.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000006.png, invalid shape: (128, 128)\n",
      "Skipping 00000013.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000084.png, invalid shape: (128, 128)\n",
      "Skipping 00000003.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000028.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000083.png, invalid shape: (128, 128)\n",
      "Skipping 00000046.png, invalid shape: (128, 128)\n",
      "Skipping 00000007.png, invalid shape: (128, 128)\n",
      "Skipping 00000020.png, invalid shape: (128, 128)\n",
      "Skipping 00000015.png, invalid shape: (128, 128)\n",
      "Skipping 00000021.png, invalid shape: (128, 128)\n",
      "Skipping 00000007.png, invalid shape: (128, 128)\n",
      "Skipping 00000063.png, invalid shape: (128, 128)\n",
      "Skipping 00000029.png, invalid shape: (128, 128)\n",
      "Skipping 00000005.png, invalid shape: (128, 128)\n",
      "Skipping 00000009.png, invalid shape: (128, 128)\n",
      "Skipping 00000059.png, invalid shape: (128, 128)\n",
      "Skipping 00000028.png, invalid shape: (128, 128)\n",
      "Skipping 00000052.png, invalid shape: (128, 128)\n",
      "Skipping 00000057.png, invalid shape: (128, 128)\n",
      "Skipping 00000112.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000027.png, invalid shape: (128, 128)\n",
      "Skipping 00000049.png, invalid shape: (128, 128)\n",
      "Skipping 00000107.png, invalid shape: (128, 128)\n",
      "Skipping 00000044.png, invalid shape: (128, 128)\n",
      "Skipping 00000064.png, invalid shape: (128, 128)\n",
      "Skipping 00000088.png, invalid shape: (128, 128)\n",
      "Skipping 00000004.png, invalid shape: (128, 128)\n",
      "Skipping 00000190.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000042.png, invalid shape: (128, 128)\n",
      "Skipping 00000023.png, invalid shape: (128, 128)\n",
      "Skipping 00000046.png, invalid shape: (128, 128)\n",
      "Skipping 00000055.png, invalid shape: (128, 128)\n",
      "Skipping 00000067.png, invalid shape: (128, 128)\n",
      "Skipping 00000081.png, invalid shape: (128, 128)\n",
      "Skipping 00000087.png, invalid shape: (128, 128)\n",
      "Skipping 00000102.png, invalid shape: (128, 128)\n",
      "Skipping 00000108.png, invalid shape: (128, 128)\n",
      "Skipping 00000127.png, invalid shape: (128, 128)\n",
      "Skipping 00000147.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000210.png, invalid shape: (128, 128)\n",
      "Skipping 5.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000204.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000205.png, invalid shape: (128, 128)\n",
      "Skipping 00000234.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000012.png, invalid shape: (128, 128)\n",
      "Skipping 00000109.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000120.png, invalid shape: (128, 128)\n",
      "Skipping 00000158.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000072.png, invalid shape: (128, 128)\n",
      "Skipping 00000139.png, invalid shape: (128, 128)\n",
      "Skipping 00000176.png, invalid shape: (128, 128)\n",
      "Skipping 00000193.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000175.png, invalid shape: (128, 128)\n",
      "Skipping 00000194.png, invalid shape: (128, 128, 2)\n",
      "Skipping 00000018.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000086.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000128.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000091.png, invalid shape: (128, 128)\n",
      "Skipping 00000072.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000013.png, invalid shape: (128, 128)\n",
      "Skipping 00000023.png, invalid shape: (128, 128)\n",
      "Skipping 00000003.png, invalid shape: (128, 128)\n",
      "Skipping 00000117.png, invalid shape: (128, 128)\n",
      "Skipping 00000178.png, invalid shape: (128, 128)\n",
      "Skipping 00000191.png, invalid shape: (128, 128)\n",
      "Skipping 00000117.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000018.png, invalid shape: (128, 128)\n",
      "Skipping 00000086.png, invalid shape: (128, 128)\n",
      "Skipping 00000089.png, invalid shape: (128, 128)\n",
      "Skipping a9e0e66523a5477fb8cb58b57d89af24.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000140.png, invalid shape: (128, 128)\n",
      "Skipping 00000156.png, invalid shape: (128, 128)\n",
      "Skipping 00000169.png, invalid shape: (128, 128)\n",
      "Skipping 00000214.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000064.png, invalid shape: (128, 128)\n",
      "Skipping 00000187.png, invalid shape: (128, 128)\n",
      "Skipping 00000011.png, invalid shape: (128, 128)\n",
      "Skipping 00000073.png, invalid shape: (128, 128)\n",
      "Skipping 00000192.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000020.png, invalid shape: (128, 128)\n",
      "Skipping 00000068.png, invalid shape: (128, 128)\n",
      "Skipping 00000109.png, invalid shape: (128, 128)\n",
      "Skipping 00000181.png, invalid shape: (128, 128)\n",
      "Skipping 00000186.png, invalid shape: (128, 128)\n",
      "Skipping 00000191.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000116.png, invalid shape: (128, 128)\n",
      "Skipping 00000040.png, invalid shape: (128, 128)\n",
      "Skipping 00000056.png, invalid shape: (128, 128)\n",
      "Skipping 00000072.png, invalid shape: (128, 128)\n",
      "Skipping 00000073.png, invalid shape: (128, 128, 2)\n",
      "Skipping 00000080.png, invalid shape: (128, 128)\n",
      "Skipping 00000086.png, invalid shape: (128, 128)\n",
      "Skipping 00000097.png, invalid shape: (128, 128)\n",
      "Skipping 00000190.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000117.png, invalid shape: (128, 128)\n",
      "Skipping 00000120.png, invalid shape: (128, 128)\n",
      "Skipping 00000184.png, invalid shape: (128, 128)\n",
      "Skipping 00000200.png, invalid shape: (128, 128)\n",
      "Skipping 00000021.png, invalid shape: (128, 128)\n",
      "Skipping e20dd0c9dbae4a299b32be5f486e4143.jpg, invalid shape: (128, 128)\n",
      "Skipping 00000106.png, invalid shape: (128, 128)\n",
      "Skipping 00000122.png, invalid shape: (128, 128)\n",
      "Skipping 00000073.png, invalid shape: (128, 128)\n",
      "Skipping 00000087.png, invalid shape: (128, 128)\n",
      "Skipping 00000088.png, invalid shape: (128, 128)\n",
      "Skipping 00000097.png, invalid shape: (128, 128)\n",
      "Skipping 00000098.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Skipping 00000010.png, invalid shape: (128, 128)\n",
      "Skipping 00000099.png, invalid shape: (128, 128)\n",
      "Skipping 00000006.png, invalid shape: (128, 128)\n",
      "Skipping 00000024.png, invalid shape: (128, 128)\n",
      "Skipping 00000070.png, invalid shape: (128, 128)\n",
      "Skipping 00000073.png, invalid shape: (128, 128, 2)\n",
      "Skipping 00000114.png, invalid shape: (128, 128)\n",
      "Skipping 00000000.png, invalid shape: (128, 128)\n",
      "Skipping 00000033.png, invalid shape: (128, 128)\n",
      "Skipping 00000034.png, invalid shape: (128, 128)\n",
      "Skipping 00000047.png, invalid shape: (128, 128)\n",
      "Skipping 00000048.png, invalid shape: (128, 128)\n",
      "Skipping 00000002.png, invalid shape: (128, 128)\n",
      "Skipping 00000001.png, invalid shape: (128, 128)\n",
      "Dataset saved at ../images_as_npz/pokemon_rgb_non_norm128x128.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]]],\n",
       " \n",
       " \n",
       "        [[[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]]],\n",
       " \n",
       " \n",
       "        [[[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]],\n",
       " \n",
       "         [[  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          ...,\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0],\n",
       "          [  0,   0,   0]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          ...,\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213]],\n",
       " \n",
       "         [[218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          ...,\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213]],\n",
       " \n",
       "         [[218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          ...,\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          ...,\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213]],\n",
       " \n",
       "         [[218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          ...,\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213]],\n",
       " \n",
       "         [[218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          ...,\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213],\n",
       "          [218, 229, 213]]],\n",
       " \n",
       " \n",
       "        [[[234, 250, 250],\n",
       "          [201, 230, 238],\n",
       "          [171, 215, 232],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]],\n",
       " \n",
       "         [[152, 203, 226],\n",
       "          [129, 194, 223],\n",
       "          [118, 198, 233],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]],\n",
       " \n",
       "         [[ 94, 201, 248],\n",
       "          [ 80, 198, 244],\n",
       "          [ 70, 199, 239],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[254, 254, 255],\n",
       "          [254, 254, 255],\n",
       "          [254, 254, 254],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]],\n",
       " \n",
       "         [[254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 252],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]],\n",
       " \n",
       "         [[254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 252],\n",
       "          ...,\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254],\n",
       "          [254, 254, 254]]],\n",
       " \n",
       " \n",
       "        [[[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       " \n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       " \n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       " \n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]],\n",
       " \n",
       "         [[255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255]]]], dtype=uint8),\n",
       " array([  0,   0,   0, ..., 142, 142, 142]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_and_label_data(folder_dir='../images/all_gen_1_images/', grayscale=False)  # Saves as \"pokemon_rgb_128x128.npz\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images with shape: (16531, 128, 128, 3)\n",
      "Loaded labels with shape: (16531,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('../images_as_npz/pokemon_rgb_non_norm128x128.npz')\n",
    "images = data['images']\n",
    "print(f\"Loaded images with shape: {images.shape}\")\n",
    "labels = data['labels']\n",
    "print(f\"Loaded labels with shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon with the least amount of labels: 82\n",
      "Number of occurrences: 45\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each unique label\n",
    "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Find the Pokémon with the least amount of labels\n",
    "min_label_index = np.argmin(label_counts)\n",
    "min_label = unique_labels[min_label_index]\n",
    "min_label_count = label_counts[min_label_index]\n",
    "\n",
    "print(f\"Pokemon with the least amount of labels: {min_label}\")\n",
    "print(f\"Number of occurrences: {min_label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.unique(labels).size\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\yoyodyne\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">143</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,295</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m143\u001b[0m)            │         \u001b[38;5;34m9,295\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,879</span> (581.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m148,879\u001b[0m (581.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,303</span> (579.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m148,303\u001b[0m (579.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> (2.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576\u001b[0m (2.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=121312)\n",
    "\n",
    "model = create_CNN_model(dropout=0.45,grayscale=False,image_size=128, num_classes=num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\yoyodyne\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.0320 - loss: 4.7720\n",
      "Epoch 1: val_accuracy improved from -inf to 0.07621, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 333ms/step - accuracy: 0.0320 - loss: 4.7712 - val_accuracy: 0.0762 - val_loss: 4.1699\n",
      "Epoch 2/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.1004 - loss: 3.8960\n",
      "Epoch 2: val_accuracy improved from 0.07621 to 0.18407, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 347ms/step - accuracy: 0.1004 - loss: 3.8957 - val_accuracy: 0.1841 - val_loss: 3.2227\n",
      "Epoch 3/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.1391 - loss: 3.5202\n",
      "Epoch 3: val_accuracy improved from 0.18407 to 0.21472, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 335ms/step - accuracy: 0.1391 - loss: 3.5200 - val_accuracy: 0.2147 - val_loss: 3.2458\n",
      "Epoch 4/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.1958 - loss: 3.1889\n",
      "Epoch 4: val_accuracy improved from 0.21472 to 0.30948, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 352ms/step - accuracy: 0.1958 - loss: 3.1888 - val_accuracy: 0.3095 - val_loss: 2.6929\n",
      "Epoch 5/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.2288 - loss: 3.0164\n",
      "Epoch 5: val_accuracy improved from 0.30948 to 0.33085, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 331ms/step - accuracy: 0.2288 - loss: 3.0163 - val_accuracy: 0.3308 - val_loss: 2.5886\n",
      "Epoch 6/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.2665 - loss: 2.8511\n",
      "Epoch 6: val_accuracy improved from 0.33085 to 0.33589, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 343ms/step - accuracy: 0.2665 - loss: 2.8510 - val_accuracy: 0.3359 - val_loss: 2.5514\n",
      "Epoch 7/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.2970 - loss: 2.7183\n",
      "Epoch 7: val_accuracy did not improve from 0.33589\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 332ms/step - accuracy: 0.2970 - loss: 2.7182 - val_accuracy: 0.2440 - val_loss: 3.0457\n",
      "Epoch 8/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - accuracy: 0.3214 - loss: 2.5972\n",
      "Epoch 8: val_accuracy improved from 0.33589 to 0.40343, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 344ms/step - accuracy: 0.3214 - loss: 2.5972 - val_accuracy: 0.4034 - val_loss: 2.3128\n",
      "Epoch 9/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - accuracy: 0.3416 - loss: 2.4877\n",
      "Epoch 9: val_accuracy improved from 0.40343 to 0.43690, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 338ms/step - accuracy: 0.3416 - loss: 2.4876 - val_accuracy: 0.4369 - val_loss: 2.1352\n",
      "Epoch 10/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.3637 - loss: 2.4047\n",
      "Epoch 10: val_accuracy improved from 0.43690 to 0.46048, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 326ms/step - accuracy: 0.3637 - loss: 2.4047 - val_accuracy: 0.4605 - val_loss: 2.0559\n",
      "Epoch 11/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.3874 - loss: 2.3219\n",
      "Epoch 11: val_accuracy improved from 0.46048 to 0.49093, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 330ms/step - accuracy: 0.3874 - loss: 2.3219 - val_accuracy: 0.4909 - val_loss: 1.9424\n",
      "Epoch 12/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.4059 - loss: 2.2001\n",
      "Epoch 12: val_accuracy did not improve from 0.49093\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 329ms/step - accuracy: 0.4059 - loss: 2.2002 - val_accuracy: 0.4294 - val_loss: 2.1591\n",
      "Epoch 13/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.4165 - loss: 2.1607\n",
      "Epoch 13: val_accuracy did not improve from 0.49093\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 322ms/step - accuracy: 0.4165 - loss: 2.1607 - val_accuracy: 0.4484 - val_loss: 2.1310\n",
      "Epoch 14/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.4291 - loss: 2.1180\n",
      "Epoch 14: val_accuracy did not improve from 0.49093\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 333ms/step - accuracy: 0.4291 - loss: 2.1180 - val_accuracy: 0.4839 - val_loss: 1.9260\n",
      "Epoch 15/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.4576 - loss: 2.0308\n",
      "Epoch 15: val_accuracy improved from 0.49093 to 0.56815, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 336ms/step - accuracy: 0.4576 - loss: 2.0309 - val_accuracy: 0.5681 - val_loss: 1.6096\n",
      "Epoch 16/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.4654 - loss: 1.9787\n",
      "Epoch 16: val_accuracy did not improve from 0.56815\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 329ms/step - accuracy: 0.4654 - loss: 1.9787 - val_accuracy: 0.5097 - val_loss: 1.8667\n",
      "Epoch 17/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.4661 - loss: 1.9698\n",
      "Epoch 17: val_accuracy did not improve from 0.56815\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 321ms/step - accuracy: 0.4661 - loss: 1.9698 - val_accuracy: 0.4798 - val_loss: 2.0655\n",
      "Epoch 18/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4943 - loss: 1.8857\n",
      "Epoch 18: val_accuracy improved from 0.56815 to 0.56976, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 382ms/step - accuracy: 0.4943 - loss: 1.8857 - val_accuracy: 0.5698 - val_loss: 1.6394\n",
      "Epoch 19/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.4924 - loss: 1.8682\n",
      "Epoch 19: val_accuracy did not improve from 0.56976\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 357ms/step - accuracy: 0.4923 - loss: 1.8683 - val_accuracy: 0.5696 - val_loss: 1.6303\n",
      "Epoch 20/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.5093 - loss: 1.8070\n",
      "Epoch 20: val_accuracy improved from 0.56976 to 0.59657, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 323ms/step - accuracy: 0.5093 - loss: 1.8071 - val_accuracy: 0.5966 - val_loss: 1.5385\n",
      "Epoch 21/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.5122 - loss: 1.8069\n",
      "Epoch 21: val_accuracy did not improve from 0.59657\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 373ms/step - accuracy: 0.5122 - loss: 1.8068 - val_accuracy: 0.5861 - val_loss: 1.5829\n",
      "Epoch 22/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 0.5270 - loss: 1.7420\n",
      "Epoch 22: val_accuracy improved from 0.59657 to 0.63790, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 402ms/step - accuracy: 0.5270 - loss: 1.7420 - val_accuracy: 0.6379 - val_loss: 1.3553\n",
      "Epoch 23/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.5191 - loss: 1.7323\n",
      "Epoch 23: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 357ms/step - accuracy: 0.5191 - loss: 1.7323 - val_accuracy: 0.6292 - val_loss: 1.4228\n",
      "Epoch 24/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364ms/step - accuracy: 0.5293 - loss: 1.7018\n",
      "Epoch 24: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 387ms/step - accuracy: 0.5293 - loss: 1.7019 - val_accuracy: 0.6069 - val_loss: 1.4805\n",
      "Epoch 25/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.5464 - loss: 1.6645\n",
      "Epoch 25: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 368ms/step - accuracy: 0.5464 - loss: 1.6646 - val_accuracy: 0.5891 - val_loss: 1.5522\n",
      "Epoch 26/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.5516 - loss: 1.6460\n",
      "Epoch 26: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 347ms/step - accuracy: 0.5516 - loss: 1.6461 - val_accuracy: 0.6262 - val_loss: 1.4548\n",
      "Epoch 27/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.5449 - loss: 1.6617\n",
      "Epoch 27: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 354ms/step - accuracy: 0.5449 - loss: 1.6617 - val_accuracy: 0.6149 - val_loss: 1.4538\n",
      "Epoch 28/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.5568 - loss: 1.5876\n",
      "Epoch 28: val_accuracy did not improve from 0.63790\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 325ms/step - accuracy: 0.5568 - loss: 1.5876 - val_accuracy: 0.6333 - val_loss: 1.3774\n",
      "Epoch 29/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.5657 - loss: 1.5774\n",
      "Epoch 29: val_accuracy improved from 0.63790 to 0.65302, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 307ms/step - accuracy: 0.5657 - loss: 1.5775 - val_accuracy: 0.6530 - val_loss: 1.2966\n",
      "Epoch 30/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.5636 - loss: 1.5783\n",
      "Epoch 30: val_accuracy did not improve from 0.65302\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 301ms/step - accuracy: 0.5636 - loss: 1.5783 - val_accuracy: 0.6147 - val_loss: 1.4611\n",
      "Epoch 31/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.5748 - loss: 1.5450\n",
      "Epoch 31: val_accuracy did not improve from 0.65302\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 302ms/step - accuracy: 0.5748 - loss: 1.5450 - val_accuracy: 0.6397 - val_loss: 1.3784\n",
      "Epoch 32/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.5784 - loss: 1.5307\n",
      "Epoch 32: val_accuracy did not improve from 0.65302\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 301ms/step - accuracy: 0.5784 - loss: 1.5307 - val_accuracy: 0.5028 - val_loss: 1.9549\n",
      "Epoch 33/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.5893 - loss: 1.4823\n",
      "Epoch 33: val_accuracy did not improve from 0.65302\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 301ms/step - accuracy: 0.5893 - loss: 1.4824 - val_accuracy: 0.6464 - val_loss: 1.3883\n",
      "Epoch 34/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.5960 - loss: 1.4881\n",
      "Epoch 34: val_accuracy improved from 0.65302 to 0.67036, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 309ms/step - accuracy: 0.5960 - loss: 1.4882 - val_accuracy: 0.6704 - val_loss: 1.2622\n",
      "Epoch 35/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.5991 - loss: 1.4792\n",
      "Epoch 35: val_accuracy improved from 0.67036 to 0.68589, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 332ms/step - accuracy: 0.5991 - loss: 1.4792 - val_accuracy: 0.6859 - val_loss: 1.2147\n",
      "Epoch 36/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.6018 - loss: 1.4301\n",
      "Epoch 36: val_accuracy did not improve from 0.68589\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 368ms/step - accuracy: 0.6018 - loss: 1.4301 - val_accuracy: 0.6339 - val_loss: 1.3756\n",
      "Epoch 37/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.6061 - loss: 1.4515\n",
      "Epoch 37: val_accuracy did not improve from 0.68589\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 370ms/step - accuracy: 0.6060 - loss: 1.4515 - val_accuracy: 0.5978 - val_loss: 1.6233\n",
      "Epoch 38/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 0.6031 - loss: 1.4196\n",
      "Epoch 38: val_accuracy did not improve from 0.68589\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 318ms/step - accuracy: 0.6031 - loss: 1.4197 - val_accuracy: 0.6800 - val_loss: 1.2267\n",
      "Epoch 39/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.6035 - loss: 1.4429\n",
      "Epoch 39: val_accuracy did not improve from 0.68589\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 340ms/step - accuracy: 0.6035 - loss: 1.4430 - val_accuracy: 0.6849 - val_loss: 1.1778\n",
      "Epoch 40/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.6058 - loss: 1.4109\n",
      "Epoch 40: val_accuracy improved from 0.68589 to 0.69536, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 304ms/step - accuracy: 0.6058 - loss: 1.4109 - val_accuracy: 0.6954 - val_loss: 1.1575\n",
      "Epoch 41/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.6153 - loss: 1.3546\n",
      "Epoch 41: val_accuracy did not improve from 0.69536\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 325ms/step - accuracy: 0.6153 - loss: 1.3548 - val_accuracy: 0.6952 - val_loss: 1.1617\n",
      "Epoch 42/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.6189 - loss: 1.3814\n",
      "Epoch 42: val_accuracy did not improve from 0.69536\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 336ms/step - accuracy: 0.6189 - loss: 1.3814 - val_accuracy: 0.6599 - val_loss: 1.2947\n",
      "Epoch 43/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - accuracy: 0.6258 - loss: 1.3319\n",
      "Epoch 43: val_accuracy did not improve from 0.69536\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 344ms/step - accuracy: 0.6257 - loss: 1.3320 - val_accuracy: 0.6931 - val_loss: 1.1632\n",
      "Epoch 44/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.6243 - loss: 1.3470\n",
      "Epoch 44: val_accuracy did not improve from 0.69536\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 350ms/step - accuracy: 0.6243 - loss: 1.3471 - val_accuracy: 0.6903 - val_loss: 1.2146\n",
      "Epoch 45/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.6350 - loss: 1.3204\n",
      "Epoch 45: val_accuracy improved from 0.69536 to 0.71310, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 347ms/step - accuracy: 0.6350 - loss: 1.3205 - val_accuracy: 0.7131 - val_loss: 1.1102\n",
      "Epoch 46/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.6203 - loss: 1.3366\n",
      "Epoch 46: val_accuracy did not improve from 0.71310\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 327ms/step - accuracy: 0.6203 - loss: 1.3366 - val_accuracy: 0.6484 - val_loss: 1.3440\n",
      "Epoch 47/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.6313 - loss: 1.3208\n",
      "Epoch 47: val_accuracy did not improve from 0.71310\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 309ms/step - accuracy: 0.6313 - loss: 1.3208 - val_accuracy: 0.7073 - val_loss: 1.1453\n",
      "Epoch 48/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.6193 - loss: 1.3522\n",
      "Epoch 48: val_accuracy did not improve from 0.71310\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 312ms/step - accuracy: 0.6193 - loss: 1.3521 - val_accuracy: 0.7093 - val_loss: 1.1318\n",
      "Epoch 49/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6336 - loss: 1.3051\n",
      "Epoch 49: val_accuracy improved from 0.71310 to 0.72944, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 299ms/step - accuracy: 0.6336 - loss: 1.3052 - val_accuracy: 0.7294 - val_loss: 1.0401\n",
      "Epoch 50/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.6384 - loss: 1.2934\n",
      "Epoch 50: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 335ms/step - accuracy: 0.6383 - loss: 1.2934 - val_accuracy: 0.7185 - val_loss: 1.1112\n",
      "Epoch 51/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.6387 - loss: 1.2826\n",
      "Epoch 51: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 337ms/step - accuracy: 0.6387 - loss: 1.2827 - val_accuracy: 0.7004 - val_loss: 1.1358\n",
      "Epoch 52/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.6505 - loss: 1.2881\n",
      "Epoch 52: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 352ms/step - accuracy: 0.6505 - loss: 1.2881 - val_accuracy: 0.7111 - val_loss: 1.1177\n",
      "Epoch 53/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 0.6423 - loss: 1.2798\n",
      "Epoch 53: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 365ms/step - accuracy: 0.6423 - loss: 1.2798 - val_accuracy: 0.7125 - val_loss: 1.1436\n",
      "Epoch 54/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.6565 - loss: 1.2528\n",
      "Epoch 54: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 352ms/step - accuracy: 0.6565 - loss: 1.2528 - val_accuracy: 0.7093 - val_loss: 1.1044\n",
      "Epoch 55/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.6471 - loss: 1.2579\n",
      "Epoch 55: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 365ms/step - accuracy: 0.6471 - loss: 1.2579 - val_accuracy: 0.6954 - val_loss: 1.2209\n",
      "Epoch 56/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step - accuracy: 0.6596 - loss: 1.2142\n",
      "Epoch 56: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 391ms/step - accuracy: 0.6596 - loss: 1.2142 - val_accuracy: 0.7286 - val_loss: 1.0645\n",
      "Epoch 57/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - accuracy: 0.6532 - loss: 1.2148\n",
      "Epoch 57: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 337ms/step - accuracy: 0.6532 - loss: 1.2149 - val_accuracy: 0.7147 - val_loss: 1.1126\n",
      "Epoch 58/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.6528 - loss: 1.2451\n",
      "Epoch 58: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 348ms/step - accuracy: 0.6528 - loss: 1.2452 - val_accuracy: 0.7198 - val_loss: 1.1051\n",
      "Epoch 59/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.6612 - loss: 1.2091\n",
      "Epoch 59: val_accuracy did not improve from 0.72944\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 342ms/step - accuracy: 0.6612 - loss: 1.2091 - val_accuracy: 0.7044 - val_loss: 1.1652\n",
      "Epoch 60/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 0.6532 - loss: 1.2589\n",
      "Epoch 60: val_accuracy improved from 0.72944 to 0.74012, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 322ms/step - accuracy: 0.6532 - loss: 1.2589 - val_accuracy: 0.7401 - val_loss: 1.0209\n",
      "Epoch 61/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.6605 - loss: 1.2113\n",
      "Epoch 61: val_accuracy improved from 0.74012 to 0.74254, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 310ms/step - accuracy: 0.6605 - loss: 1.2113 - val_accuracy: 0.7425 - val_loss: 1.0206\n",
      "Epoch 62/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6552 - loss: 1.2138\n",
      "Epoch 62: val_accuracy did not improve from 0.74254\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 298ms/step - accuracy: 0.6552 - loss: 1.2138 - val_accuracy: 0.7397 - val_loss: 1.0288\n",
      "Epoch 63/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.6655 - loss: 1.1868\n",
      "Epoch 63: val_accuracy did not improve from 0.74254\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 300ms/step - accuracy: 0.6655 - loss: 1.1869 - val_accuracy: 0.7111 - val_loss: 1.1406\n",
      "Epoch 64/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.6639 - loss: 1.2038\n",
      "Epoch 64: val_accuracy did not improve from 0.74254\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 300ms/step - accuracy: 0.6639 - loss: 1.2038 - val_accuracy: 0.7411 - val_loss: 1.0023\n",
      "Epoch 65/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6630 - loss: 1.2116\n",
      "Epoch 65: val_accuracy improved from 0.74254 to 0.74375, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 298ms/step - accuracy: 0.6630 - loss: 1.2116 - val_accuracy: 0.7437 - val_loss: 1.0175\n",
      "Epoch 66/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.6706 - loss: 1.1832\n",
      "Epoch 66: val_accuracy did not improve from 0.74375\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 303ms/step - accuracy: 0.6706 - loss: 1.1833 - val_accuracy: 0.7417 - val_loss: 1.0190\n",
      "Epoch 67/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.6695 - loss: 1.1754\n",
      "Epoch 67: val_accuracy did not improve from 0.74375\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 302ms/step - accuracy: 0.6695 - loss: 1.1754 - val_accuracy: 0.7077 - val_loss: 1.1696\n",
      "Epoch 68/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6643 - loss: 1.1933\n",
      "Epoch 68: val_accuracy did not improve from 0.74375\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 298ms/step - accuracy: 0.6643 - loss: 1.1932 - val_accuracy: 0.6919 - val_loss: 1.1709\n",
      "Epoch 69/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.6646 - loss: 1.1781\n",
      "Epoch 69: val_accuracy improved from 0.74375 to 0.74738, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 298ms/step - accuracy: 0.6646 - loss: 1.1781 - val_accuracy: 0.7474 - val_loss: 0.9953\n",
      "Epoch 70/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6685 - loss: 1.1577\n",
      "Epoch 70: val_accuracy did not improve from 0.74738\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 298ms/step - accuracy: 0.6685 - loss: 1.1578 - val_accuracy: 0.7446 - val_loss: 1.0097\n",
      "Epoch 71/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.6732 - loss: 1.1593\n",
      "Epoch 71: val_accuracy did not improve from 0.74738\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 296ms/step - accuracy: 0.6732 - loss: 1.1594 - val_accuracy: 0.7442 - val_loss: 0.9853\n",
      "Epoch 72/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.6703 - loss: 1.1495\n",
      "Epoch 72: val_accuracy did not improve from 0.74738\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 297ms/step - accuracy: 0.6703 - loss: 1.1495 - val_accuracy: 0.7304 - val_loss: 1.0474\n",
      "Epoch 73/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.6821 - loss: 1.1125\n",
      "Epoch 73: val_accuracy improved from 0.74738 to 0.75605, saving model to ./models/all_gen_1_rgb.keras\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 296ms/step - accuracy: 0.6821 - loss: 1.1126 - val_accuracy: 0.7560 - val_loss: 1.0128\n",
      "Epoch 74/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.6684 - loss: 1.1645\n",
      "Epoch 74: val_accuracy did not improve from 0.75605\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 298ms/step - accuracy: 0.6684 - loss: 1.1645 - val_accuracy: 0.7155 - val_loss: 1.1130\n",
      "Epoch 75/75\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.6813 - loss: 1.1098\n",
      "Epoch 75: val_accuracy did not improve from 0.75605\n",
      "\u001b[1m362/362\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 296ms/step - accuracy: 0.6813 - loss: 1.1099 - val_accuracy: 0.7351 - val_loss: 1.0437\n"
     ]
    }
   ],
   "source": [
    "model = CNN_fit_train(model, X_train,y_train,X_test,y_test,num_epochs=75,num_batch=32, save_path='./models/all_gen_1_rgb.keras', datagen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoyodyne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
